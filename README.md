## CRM Tools Evaluation

The goal of this repository is to compare different tools collections for Agents to work with CRMs.

Goal and how in points (TBD writing after numbers):
- Evaluate minimalistic agent to see effect of tools on performance
- Compare different models to eliminate model effect
- Focus to pass^k measuring consistency (pass hat k) [^2] metric
- Validating state in the HubSpot
- API mocking not possible, agent actions (different steps, order, ...) 

Toolsets:
- Superface Specialists
- Composio
- Arcade
- Zapier
- Nango


## Setup
- Create Private App in Hubpot with all CRM scopes
  - plus: sales-email-read

---

[^1] Gorilla: Large Language Model Connected with Massive APIs https://arxiv.org/abs/2305.15334
[^2] Ï„-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains https://arxiv.org/abs/2406.12045
[^3] AgentBench: Evaluating LLMs as Agents https://arxiv.org/abs/2308.03688 


---

- Would it be possible to use screenshots to evaluate results?
